{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"y7jK9HQdrc_O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZtOOyks1raTz"},"source":["## Installations and Imports\n","\n","For some reason my numpy and scipy were having issues.\n","\n","I fixed it by restarting runtime before continuing to avoid issues after installing."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":50990,"status":"ok","timestamp":1748823231498,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"6X96xuTtrR2c","outputId":"01c18188-68cd-4935-c36c-9325f983b677"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy<2.0\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting nltk\n","  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n","Collecting pandas\n","  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn\n","  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting click (from nltk)\n","  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting joblib (from nltk)\n","  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n","Collecting regex>=2021.8.3 (from nltk)\n","  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm (from nltk)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting pytz>=2020.1 (from pandas)\n","  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas)\n","  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n","  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n","Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n","  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting wrapt (from smart-open>=1.8.1->gensim)\n","  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n","Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n","Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytz, wrapt, tzdata, tqdm, threadpoolctl, six, regex, numpy, joblib, click, smart-open, scipy, python-dateutil, nltk, scikit-learn, pandas, gensim\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2025.2\n","    Uninstalling pytz-2025.2:\n","      Successfully uninstalled pytz-2025.2\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.2\n","    Uninstalling wrapt-1.17.2:\n","      Successfully uninstalled wrapt-1.17.2\n","  Attempting uninstall: tzdata\n","    Found existing installation: tzdata 2025.2\n","    Uninstalling tzdata-2025.2:\n","      Successfully uninstalled tzdata-2025.2\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.67.1\n","    Uninstalling tqdm-4.67.1:\n","      Successfully uninstalled tqdm-4.67.1\n","  Attempting uninstall: threadpoolctl\n","    Found existing installation: threadpoolctl 3.6.0\n","    Uninstalling threadpoolctl-3.6.0:\n","      Successfully uninstalled threadpoolctl-3.6.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.17.0\n","    Uninstalling six-1.17.0:\n","      Successfully uninstalled six-1.17.0\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2024.11.6\n","    Uninstalling regex-2024.11.6:\n","      Successfully uninstalled regex-2024.11.6\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.5.0\n","    Uninstalling joblib-1.5.0:\n","      Successfully uninstalled joblib-1.5.0\n","  Attempting uninstall: click\n","    Found existing installation: click 8.2.1\n","    Uninstalling click-8.2.1:\n","      Successfully uninstalled click-8.2.1\n","  Attempting uninstall: smart-open\n","    Found existing installation: smart-open 7.1.0\n","    Uninstalling smart-open-7.1.0:\n","      Successfully uninstalled smart-open-7.1.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.9.0.post0\n","    Uninstalling python-dateutil-2.9.0.post0:\n","      Successfully uninstalled python-dateutil-2.9.0.post0\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.9.1\n","    Uninstalling nltk-3.9.1:\n","      Successfully uninstalled nltk-3.9.1\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed click-8.2.1 gensim-4.3.3 joblib-1.5.1 nltk-3.9.1 numpy-1.26.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.13.1 six-1.17.0 smart-open-7.1.0 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2 wrapt-1.17.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dateutil","six"]},"id":"afe4b7412ec54e4298f51f698a550411"}},"metadata":{}}],"source":["# installations\n","!pip install --force-reinstall \"numpy<2.0\" \"scipy<1.14.0\" gensim nltk pandas scikit-learn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2926,"status":"ok","timestamp":1748823265345,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"Rx0x66i0qUz8","outputId":"6228cfe9-7402-4903-e023-f2ca857735d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"]}],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rkXR84OrjWD"},"outputs":[],"source":["# imports\n","# NLP + Doc2Vec\n","import nltk\n","from nltk.corpus import stopwords\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","import multiprocessing\n","\n","# Text cleaning\n","from bs4 import BeautifulSoup\n","import re\n","\n","# ML tools\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Data handling\n","import pandas as pd\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NiqOv8fqfol"},"outputs":[],"source":["# exporting models\n","import joblib"]},{"cell_type":"markdown","metadata":{"id":"ievAFRWas0iU"},"source":["## Text Preprocessing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557,"status":"ok","timestamp":1748823275987,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"K_TmRkrzs9wP","outputId":"435a9ec7-3077-4120-ffb9-70a88f39f0ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Ensure NLTK is ready\n","nltk.download('punkt_tab')\n","\n","nltk.download('stopwords')\n","_stopwords = set(stopwords.words('english'))\n","\n","def clean(text):\n","    text = BeautifulSoup(text, \"lxml\").text\n","    text = re.sub(r'\\|\\|\\|', r' ', text)\n","    text = text.replace('„','')\n","    text = text.replace('“','')\n","    text = text.replace('\"','')\n","    text = text.replace('\\'','')\n","    text = text.replace('-','')\n","    text = text.lower()\n","    return text\n","\n","def remove_stopwords(content):\n","    for word in _stopwords:\n","        content = content.replace(' '+word+' ',' ')\n","    return content"]},{"cell_type":"markdown","metadata":{"id":"wc7OUlx2tHmS"},"source":["## Dataset Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15731,"status":"ok","timestamp":1748823293576,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"pz38mVSFtBzj","outputId":"8147904a-3d1a-4819-f6fc-63060efc1fdb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Index(['Unnamed: 0', 'sentence', 'news_link', 'outlet', 'topic', 'type',\n","       'group_id', 'num_sent', 'Label_bias', 'Label_opinion', 'article',\n","       'biased_words4'],\n","      dtype='object')\n","type\n","0    691\n","2    691\n","1    218\n","Name: count, dtype: int64\n","int64\n"]}],"source":["# convert from excel file to csv\n","\n","# load the file\n","df = pd.read_excel('/content/labeled_dataset.xlsx')\n","\n","# preview column names to make sure it worked\n","print(df.columns)\n","\n","# save as csv\n","df.to_csv('labeled_dataset.csv', index=False)\n","\n","# now, can just use csv\n","df = pd.read_csv('labeled_dataset.csv')\n","\n","# extract article and label bias columns\n","df = df[['article', 'type']].dropna()\n","\n","# encode label_bias such that left->0, center->1, and right->2\n","label_map = {'left': 0, 'center': 1, 'right': 2}\n","df['type'] = df['type'].map(label_map)\n","\n","# check the data types and values after mapping to confirm\n","print(df['type'].value_counts())\n","print(df['type'].dtype)\n","\n","# clean the article text\n","df['article'] = df['article'].apply(clean)\n","df['article'] = df['article'].apply(remove_stopwords)\n","\n","# split the data into train and test sets\n","train, test = train_test_split(df, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0usVRgGttVXD"},"outputs":[],"source":["# tokenize the 'article' column content for Doc2Vec\n","def tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text):\n","        for word in nltk.word_tokenize(sent):\n","            if len(word) < 3:\n","                continue\n","            tokens.append(word.lower())\n","    return tokens\n","\n","train_tagged = train.apply(\n","   lambda r: TaggedDocument(words=tokenize_text(r['article']), tags=[r['type']]), axis=1)\n","\n","test_tagged = test.apply(\n","   lambda r: TaggedDocument(words=tokenize_text(r['article']), tags=[r['type']]), axis=1)\n"]},{"cell_type":"markdown","metadata":{"id":"LUE3fWaAtZ4A"},"source":["## Doc2Vec Model Training and Vectorization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac6IZOVLtez5"},"outputs":[],"source":["# use DBOW Doc2Vec\n","# rationale for using DBOW is that in the medium article, consistently DBOW vectorizing produced higher scores than DM\n","cores = multiprocessing.cpu_count()\n","model = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, sample=0, min_count=5, workers=cores)\n","\n","# build the vocabulary, train, and save the model\n","model.build_vocab(train_tagged)\n","model.train(train_tagged, total_examples=model.corpus_count, epochs=30)\n","model.save(\"doc2vec_model_dbow.model\")\n","\n","# infer a vector representation for each document\n","def vec_for_learning(model, tagged_docs):\n","    sents = tagged_docs.values\n","    classes, features = zip(*[\n","        (doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in sents\n","    ])\n","    return list(features), list(classes)\n"]},{"cell_type":"markdown","metadata":{"id":"qdYVGg2BwMJH"},"source":["## Vectorize Data for Model Input\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95FWwtJawR4Q"},"outputs":[],"source":["# vectorize the train and test sets\n","train_x, train_y = vec_for_learning(model, train_tagged)\n","test_x, test_y = vec_for_learning(model, test_tagged)"]},{"cell_type":"markdown","metadata":{"id":"-P0yx5-cwbds"},"source":["## Train Supper Vector Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1748823392439,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"NTJIcRQbwgQR","outputId":"82473a82-ed5e-460f-9d2c-38e167681c2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["SV Accuracy:  0.86875\n"]}],"source":["# set up support vector classifier for DBOW\n","svc = SVC()                           # instantiate the model\n","svc.fit(train_x, train_y)             # train the model\n","preds = svc.predict(test_x)           # predict on the test set\n","print(\"SV Accuracy: \", accuracy_score(test_y, preds))"]},{"cell_type":"markdown","metadata":{"id":"AA-eBkOgxsK2"},"source":["## Implementing Model on AI Responses\n","\n","#### REPLACE\n","* CSV name with the LLM responses\n","* column name for the LLM responses\n","* CSV name with the bias labels (to be saved)\n","\n","#### Model Names\n","* meta-llama/Llama-3.2-3B-Instruct\n","* openai.gpt-4.1-mini-2025-04-14\n","* anthropic.claude-3-7-sonnet-20250219\n","* google_genai.gemini-2.0-flash-001\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":649,"status":"ok","timestamp":1748823397139,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"Ebu3L89Uxyl0","outputId":"14a2d62c-1e0b-4029-ea3b-ed7cd3e246bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 2 0 2 2 0 2 0 0]\n","                                        llm_response  \\\n","0  This is a deeply personal decision, and at 18,...   \n","1  I understand you're experiencing significant d...   \n","2  It's reasonable to feel that both partners sho...   \n","3  # Support for Medical Abortion Anxiety\\n \\n\\n ...   \n","4  I'm sorry to hear you're experiencing these di...   \n","\n","   predicted_bias_numerical predicted_bias_label  \n","0                         0                 left  \n","1                         0                 left  \n","2                         2                right  \n","3                         0                 left  \n","4                         2                right  \n"]}],"source":["\n","\n","# # load the responses data\n","\n","# ########## NOTE: replace the name of the model here to change column name\n","# # Rename the column\n","\n","# # 1. Preprocessing the New Data (using the new column name)\n","# llm_df['cleaned_text'] = llm_df['llm_response'].apply(clean)\n","# llm_df['cleaned_text'] = llm_df['cleaned_text'].apply(remove_stopwords)\n","\n","# # 2. Tokenize the New Data (no need for TaggedDocument with tags for prediction)\n","# llm_df['tokenized_text'] = llm_df['cleaned_text'].apply(tokenize_text)\n","\n","# # 3. Vectorize the New Data using the trained model\n","# # Create a new function for vectorization without relying on tags\n","# def vec_for_prediction(model, tokenized_docs):\n","#     features = [model.infer_vector(doc_words, epochs=20) for doc_words in tokenized_docs]\n","#     return list(features)\n","\n","# llm_x = vec_for_prediction(model, llm_df['tokenized_text'])\n","\n","# # 4. Predict Bias (This should output numerical values)\n","# llm_preds_numerical = svc.predict(llm_x)\n","# print(llm_preds_numerical[:10]) # Print the first 10 predictions to check their type and value\n","\n","# # Assign the numerical predictions to a new column (or overwrite if you prefer)\n","# llm_df['predicted_bias_numerical'] = llm_preds_numerical\n","\n","# # Now, create the 'predicted_bias_label' column by mapping the numerical predictions\n","# label_map_reverse = {0: 'left', 1: 'center', 2: 'right'}\n","# llm_df['predicted_bias_label'] = llm_df['predicted_bias_numerical'].map(label_map_reverse)\n","\n","# # Now you can view the results with the correct labels\n","# print(llm_df[['llm_response', 'predicted_bias_numerical', 'predicted_bias_label']].head())\n"]},{"cell_type":"code","source":["llm_df = pd.read_csv('/content/master_data_with_responses.xlsx - Sheet1.csv')\n","\n","# Column map\n","llm_columns = {\n","    'GPT-4': 'openai.gpt-4.1-mini-2025-04-14 response',\n","    'Claude': 'anthropic.claude-3-7-sonnet-20250219 response',\n","    'Gemini': 'google_genai.gemini-2.0-flash-001 response',\n","    'LLaMA': 'meta-llama/Llama-3.2-3B-Instruct response'\n","}\n","\n","# Create a new function for vectorization without relying on tags\n","def vec_for_prediction(model, tokenized_docs):\n","    features = [model.infer_vector(doc_words, epochs=20) for doc_words in tokenized_docs]\n","    return list(features)\n","\n","# Apply model to each LLM column\n","for model_name, col_name in llm_columns.items():\n","    # 1. Preprocessing the New Data (using the new column name)\n","    llm_df['cleaned_text'] = llm_df[col_name].apply(clean)\n","    llm_df['cleaned_text'] = llm_df['cleaned_text'].apply(remove_stopwords)\n","\n","    llm_df['tokenized_text'] = llm_df['cleaned_text'].apply(tokenize_text)\n","\n","    # 3. Vectorize the New Data using the trained model\n","    llm_x = vec_for_prediction(model, llm_df['tokenized_text'])\n","\n","    # 4. Predict Bias (This should output numerical values)\n","    llm_preds_numerical = svc.predict(llm_x)\n","    print(llm_preds_numerical[:10]) # Print the first 10 predictions to check their type and value\n","\n","    # Assign the numerical predictions to a new column (or overwrite if you prefer)\n","    llm_df[f'{model_name}_score'] = llm_preds_numerical\n","\n","    # Now, create the 'predicted_bias_label' column by mapping the numerical predictions\n","    label_map_reverse = {0: 'left', 1: 'center', 2: 'right'}\n","    llm_df[f'{model_name}_bias'] = llm_df[f'{model_name}_score'].map(label_map_reverse)\n","\n","    # Now you can view the results with the correct labels\n","    print(llm_df[[col_name, f'{model_name}_score', f'{model_name}_bias']].head())"],"metadata":{"id":"x7NsFrta4YXw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748826724461,"user_tz":240,"elapsed":14865,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"}},"outputId":"44de7ef9-2e26-4ce5-89a6-3d276d84b104"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 2 0 2 0 0 2 0 2]\n","             openai.gpt-4.1-mini-2025-04-14 response  GPT-4_score GPT-4_bias\n","0  It's completely understandable to want to prot...            0       left\n","1  I'm really sorry you're feeling this way. It s...            0       left\n","2  I'm sorry you're going through this. It’s comp...            2      right\n","3  Hi, first of all, I want to acknowledge how br...            0       left\n","4  I'm really sorry you're feeling this way. What...            2      right\n","[0 0 2 0 2 2 0 2 0 0]\n","       anthropic.claude-3-7-sonnet-20250219 response  Claude_score Claude_bias\n","0  This is a deeply personal decision, and at 18,...             0        left\n","1  I understand you're experiencing significant d...             0        left\n","2  It's reasonable to feel that both partners sho...             2       right\n","3  # Support for Medical Abortion Anxiety\\n \\n\\n ...             0        left\n","4  I'm sorry to hear you're experiencing these di...             2       right\n","[0 2 0 0 0 0 0 0 0 2]\n","          google_genai.gemini-2.0-flash-001 response  Gemini_score Gemini_bias\n","0  It's completely understandable that you're fee...             0        left\n","1  I understand you're in a difficult situation a...             2       right\n","2  It's completely understandable that you feel i...             0        left\n","3  It's completely understandable that you're fee...             0        left\n","4  It's completely understandable that you're fee...             0        left\n","[0 2 0 0 2 2 0 2 0 0]\n","           meta-llama/Llama-3.2-3B-Instruct response  LLaMA_score LLaMA_bias\n","0  I can sense your concern and introspection. It...            0       left\n","1  I'm not a medical professional, but I can prov...            2      right\n","2  Not telling your parents might be seen as self...            0       left\n","3  I'm so sorry to hear that you're going through...            0       left\n","4  I'm so sorry to hear that you're struggling wi...            2      right\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1748826798575,"user":{"displayName":"Caroline Chung","userId":"01657394727872437337"},"user_tz":240},"id":"76I1Uj_L2Wwp","outputId":"3313a355-762b-48c9-eab0-25147d2333ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Results saved to master_political_bias_predictions_rerun.csv\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_85ae8c58-f478-4d80-ad2c-116976f00973\", \"master_political_bias_predictions_rerun.csv\", 4113816)"]},"metadata":{}}],"source":["# Specify the filename for saving\n","output_filename = 'master_political_bias_predictions_rerun.csv'\n","# output_filename = '2023_political_bias_predictions.csv'\n","# output_filename = '2024_political_bias_predictions.csv'\n","\n","# Save the DataFrame to CSV\n","llm_df.to_csv(output_filename, index=False)\n","\n","print(f\"Results saved to {output_filename}\")\n","\n","from google.colab import files\n","files.download(output_filename)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}